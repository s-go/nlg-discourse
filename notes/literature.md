# Literatur

* [x] Dimitromanolaki, A., & Androutsopoulos, I. (2003). Learning to Order Facts for Discourse Planning in Natural Language Generation. In Proc. ENLG’03, pp. 23–30.
* [ ] Duboue, P. A., & McKeown, K. R. (2002, July). Content planner construction via evolutionary algorithms and a corpus-based fitness function. In Proceedings of INLG 2002 (pp. 89-96).
* [x] Gatt, Albert; Krahmer, Emiel (2017): *Survey of the State of the Art in Natural Language Generation: Core tasks, applications and evaluation*.
> To account for discourse relations between messages, researchers have alternatively relied on Rhetorical Structure Theory (rst; e.g., Mann & Thompson, 1988; Scott & Sieckenius de Souza, 1990; Hovy, 1993), which also typically involved domain-specific rules. For example, Williams and Reiter (2008) used rst relations to identify ordering among messages that would maximise clarity to low-skilled readers.
>
> For statistical, end-to-end generation in other domains, there is less of an embarrassment of riches. However, this situation is improving as methods to automatically align input data with output text are developed.
* [ ] **Hovy, E. H. (1993). Automated discourse generation using discourse structure relations. Artificial intelligence, 63(1), 341-385.**
	* general assertions about the structure of plan-based English
discourse (12)
* [ ] Hovy, E. H. (1993). Automated Discourse Planning and Generation. In Proc. Anunal Meeting of the Society for Text and Discourse.
* [ ] Kosseim/Lapalme (1998) Choosing Rhetorical Structures to Plan Instructional Texts
* [x] **Malmi, E., Pighin, D., Krause, S., & Kozhevnikov, M. (2017). Automatic Prediction of Discourse Connectives. arXiv preprint arXiv:1702.00992.**
* [ ] Nakatsu, C., & White, M. (2010). Generating with Discourse Combinatory Categorial Grammar. Linguistic Issues in Language Technology, September.
* [x] Reiter/Dale (1997) Building Applied Natural Language Generation Systems
> 5.2 Discourse Planning
> 
> There is no consensus in the research literature on what specific discourse relations should be used in an nlg system. Probably the most commonly used set is that suggested by Rhetorical Structure Theory (RST) (Mann and Thompson 1988), although many developers modify this set to cater for idiosyncrasies of their particular domain and genre. For a general discussion of different ways of classifying discourse relations, see (Maier and Hovy 1993).
> 
> This fact, in combination with the computational expense of
planning-based approaches and the large amounts of knowledge they require, means that approaches based on these ideas are not widely used in current real-world NLG systems.
* [x] Rösner, D., & Stede, M. (1992). Customizing RST for the automatic production of technical manuals. In Aspects of automated natural language generation (pp. 199-214). Springer Berlin Heidelberg.
* [x] Scheffler, T., & Stede, M. (2016). Adding Semantic Relations to a Large-Coverage Connective Lexicon of German. In Proceedings of the Tenth International Conference on Language Resources and Evaluation (LREC 2016), Portoroz, Slovenia, May.
> RST defines relations between text segments (elementary or complex discourse units), based on the cognitive effect of the combination on the reader. Though connectives may be present, and sometimes signal relations, they are not considered central for determining the identified relation.
* [x] Stede, M., & Umbach, C. (1998). DiMLex: A lexicon of discourse markers for text generation and understanding. In Proceedings of the 17th international conference on Computational linguistics-Volume 2 (pp. 1238-1242). Association for Computational Linguistics.
* [ ] Taboada, M., & Mann, W. C. (2006). Applications of rhetorical structure theory. Discourse studies, 8(4), 567-588.

## Aligning data and text / Learning Semantic Correspondences

* [ ] Barzilay, R., & Lapata, M. (2005). Collective content selection for concept-to-text generation. In Proc. HLT/EMNLP05, pp. 331338.
* [ ] Liang, P., Jordan, M. I., & Klein, D. (2009). Learning Semantic Correspondences with Less Supervision. In Proc. ACL-IJCNLP’09, pp. 91–99.
* [ ] Koncel-Kedziorski, R., & Hajishirzi, H. (2014). Multi-Resolution Language Grounding with Weak Supervision. In Proc. EMNLP’14, pp. 386–396.

### Usage in Generation

* [x] Angeli, G., Liang, P., & Klein, D. (2010). A simple domain-independent probabilistic approach to generation. In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pp. 502-512, Cambridge, MA.
* [ ] Kim, J., & Mooney, R. (2010). Generative alignment and semantic parsing for learning from ambiguous supervision. In Proceedings of the 23rd Conference on Computational Linguistics, pp. 543-551, Beijing, China.
* [ ] Konstas, I., & Lapata, M. (2013). A global model for concept-to-text generation. Journal of Artificial Intelligence Research, 48, 305-346.

## Vielleicht interessant

* [ ] White and Howcroft (2015)

## Eher nicht

* [x] Theune, M., Hielkema, F., & Hendriks, P. (2006). Performing aggregation and ellipsis using discourse structures. Research on Language and Computation, 4, 353-375.
* [x] Vander Linden, K., Cumming, S., & Martin, J. (1992). Using system networks to build rhetorical structures. In Aspects of Automated Natural Language Generation (pp. 183-198). Springer Berlin Heidelberg.